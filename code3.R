set.seed(123)
library(car)
library(nnet)
library(GGally)
library(lmtest)
require(hash)
library(Hmisc)
library(hier.part)
library(MASS)
library(corrplot)
library(rockchalk)
library(olsrr)
library(caret)
library(MASS)
library(glmnet)
library(GlmSimulatoR)
library(GLMMadaptive)
library(faraway)
library(ggfortify)#For graphics

nn= 30
# Step 1: Generating covariance matrix
generate.cov <- function ( p =15, rho =0.0, tau =1, nn =60,
                           homogeneous = TRUE )
{
  # p is the dimensionality of the data
  # rho is the correlation coefficient
  # tau is the overall level of relatedness
  # nn is the sample size
  library ( MASS )
  pp <- 1: p
  m1p <- matrix (rep (pp , p ) , ncol =p , byrow = T )
  mp1 <- matrix (rep( pp , p ) , ncol =p , byrow = F )
  M.het <- abs( m1p - mp1)
  M.hom <- rep(1,p ) %*% t( rep(1,p ) ) # creating a matrix of all 1 s .
  
  #%*% is used for matrix multiplication
  # Creating a covariance matrix .
  if ( homogeneous )
  {
    Sigmap <- rho^M.hom ;
    diag ( Sigmap ) <- rep(1,p )
  }
  else
  {
    
    Sigmap <- rho^M.het
  }
  return(Sigmap)
}


# Simulating the three covariance matrices using the function created above
covariance_matrix_1 <- generate.cov( rho =0.5)
covariance_matrix_2 <- generate.cov( rho =0.5)
covariance_matrix_3 <- generate.cov( rho =0.5)

# Creating the Zero matrices required to create the Block Covariance matrix
#( Intrinsic matrix ).
zero_1 <-mat.or.vec (30,15)
zero_2 <-mat.or.vec (15,15)


# Appending the matrices to create the covaraince matrix .
mat1 <- rbind ( covariance_matrix_1, zero_1)
mat21 <-rbind ( zero_2, covariance_matrix_2)
mat2 <-rbind (mat21, zero_2)
mat3 <-rbind ( zero_1, covariance_matrix_3)


# Creating the Block Covariance Matrix
Block_covariance_matrix <- cbind ( mat1,mat2,mat3)

# Step 2: Generating data using the block covaraince matrix created above
p =45
X <- data.frame ( mvrnorm ( nn , mu =(1/1: p ) , Sigma = Block_covariance_matrix ) )

Y <- 1+ 2* X [ ,1] + X [ ,3] - 2.5 * X[ ,6] - 2* X [ ,8] + 7/8* X [ ,12]+ 2/5 * X [ ,13] +
  X [ ,14] + X [ ,16] + 2* X [ ,18] -3.0* X [ ,19] +
  X [ ,20] +2* X [ ,21] + X [ ,23] + 1.5* X [ ,30] - X [ ,31] + X [ ,32] +
  X [ ,33]+ 2* X [ ,34] - 0.25* X [ ,35]+ 0.85* X [ ,36] +4* X [ ,44]+ 3*rnorm ( nn )

Data.matrix <- data.frame (Y,X)


# Checking the correlation plot for the block covaraince matrix

corrplot (cor ( Block_covariance_matrix ) )
# checking the correlation plot for the Data matrix generated by inducing correlation .
corrplot (cor ( Data.matrix ) )
model.x <- as.matrix ( Data.matrix [ , -1])
model.y <- as.matrix ( Data.matrix [ ,1])


model.x <-as.matrix ( Data.matrix [ , -1])
model.y <- as.matrix ( Data.matrix [ ,1])

#_______________________________________________________
#            Forward Selection
#_______________________________________________________________________________
train.model <- lm(formula = y.train ~1, data = train.set)
train.scope <- as.formula(lm(y.train ~., data = train.set))
forward_train <- step(train.model, scope = train.scope, direction = "forward", trace = 0)
#calculating rmse on training set
print(paste("RMSE on training set :", rmse(forward_train)))
#______________________________________________________________________________________________________

# create empty list object for further usage
model.outcome = list()
X<- data.matrix(Data.matrix[c(2:46)])
Y <- Data.matrix$Y
n <- nrow(X)
train_rows <- sample(1:n, .70*n) ## 70% training, 30% test
X.train <- X[train_rows, ]
X.test <- X[-train_rows, ]
y.train <- Y[train_rows]
y.test <- Y[-train_rows]
train.set <- data.frame(y.train ,X.train)

test.set <- data.frame(y.test,X.test)
rmse2 = function(obs, preds){
  sqrt(sum((obs-preds)^2)/length(obs))
}
rmse = function(model){
  sqrt(sum((model$residual)^2)/nrow(model$model))
}
#_______________________________________________________________________________
# Ridge regression
#_______________________________________________________________________________
### RIDGE REGRESSION ###
m.ridge <- glmnet(X.train, y.train, alpha=0)
# plot model coefficients vs. shrinkage parameter lambda
plot(m.ridge, xvar = "lambda", label = TRUE)
plot(m.ridge, xvar = "dev", label = TRUE)

grid = 10^seq(10,-1,length=30) ## set lambda sequence
m.ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0, lambda = grid)
plot(m.ridge.cv)
m.ridge.cv$lambda.min

m.ridge.cv$lambda.1se
# prediction for the training set
m.ridge.cv.pred.train = predict(m.ridge.cv, X.train, s = "lambda.min")
# calculate rmse on training set
print(paste('RMSE on training set:', rmse2(m.ridge.cv.pred.train, y.train)))
# prediction for the test set
m.ridge.cv.pred.test = predict(m.ridge.cv, X.test, s = "lambda.min")
# calculate RMSE for the test data set
print(paste('RMSE on test set:', rmse2(m.ridge.cv.pred.test, y.test)))
# store model object and results of rmse in the `list` object named `model.outcome`
model.outcome[['m.ridge.cv']] = list('model' = m.ridge.cv, 
                                     'rmse' =  data.frame('name'= 'ridge regression',
                                                          'train.RMSE' = rmse2(m.ridge.cv.pred.train, y.train),
                                                          'test.RMSE' = rmse2(m.ridge.cv.pred.test, y.test)))

#R_squared for train data
m.Ridge.cv_Tr <- cv.glmnet(X.train, y.train, alpha = 0)
Ridge_model_Tr <- glmnet(X.train,y.train ,  family = "gaussian",  alpha=0,  lambda = m.Ridge.cv_Tr$lambda.min )
Ridge_model_Tr$dev.ratio

#R-squared for test data
m.Ridge.cv_T <- cv.glmnet(X.test, y.test, alpha = 0)
Ridge_model_T <- glmnet(X.test,y.test ,  family = "gaussian",  alpha=0,  lambda = m.Ridge.cv_T$lambda.min )
Ridge_model_T$dev.ratio

#________________________________________________________________________________________________________________

#_________________________________________________________________________________________________________________

#_______________________________________________________________________________________________________________
#LASSO Regression
#_______________________________________________________________________________________________________________

### LASSO REGRESSION ###
m.lasso <- glmnet(X.train, y.train, alpha = 1)

# plot model coefficients vs. shrinkage parameter lambda

plot(m.lasso, xvar = "lambda", label = TRUE)

plot(m.lasso, xvar = "dev", label = TRUE)

m.lasso.cv <- cv.glmnet(X.train, y.train, alpha = 1)

plot(m.lasso.cv)

m.lasso.cv$lambda.min

m.lasso.cv$lambda.1se
# prediction for the training set
m.lasso.cv.pred.train = predict(m.lasso.cv, X.train, s = m.lasso.cv$lambda.min)
# calculate rmse on training set
print(paste('RMSE on training set:', rmse2(m.lasso.cv.pred.train, y.train)))
# prediction for the test set
m.lasso.cv.pred.test = predict(m.lasso.cv, X.test, s = m.lasso.cv$lambda.min)
# calculate RMSE for the test data set
print(paste('RMSE on test set:', rmse2(m.lasso.cv.pred.test, y.test)))
# store model object and results of rmse in the `list` object named `model.outcome`
model.outcome[['m.lasso.cv']] = list('model' = m.lasso.cv, 
                                     'rmse' =  data.frame('name'= 'LASSO regression',
                                                          'train.RMSE' = rmse2(m.lasso.cv.pred.train, y.train),
                                                          'test.RMSE' = rmse2(m.lasso.cv.pred.test, y.test)))

#R_squared for train data
m.lasso.cv_Tr <- cv.glmnet(X.train, y.train, alpha = 1)
lasso_model_Tr <- glmnet(X.train,y.train ,  family = "gaussian",  alpha=1,  lambda = m.lasso.cv_Tr$lambda.min )
lasso_model_Tr$dev.ratio

#R-squared for test data
m.lasso.cv_T <- cv.glmnet(X.test, y.test, alpha = 1)
lasso_model_T <- glmnet(X.test,y.test ,  family = "gaussian",  alpha=1,  lambda = m.lasso.cv_T$lambda.min )
lasso_model_T$dev.ratio
#___________________________________________________________________________________________________________________________

#____________________________________________________________________________________________________________________________

E_net <- glmnet(X.train, y.train, alpha = 0.5)

m.Enet.cv <- cv.glmnet(X.train, y.train, alpha = 0.5)
m.Enet.cv$lambda.min

m.Enet.cv$lambda.1se
# prediction for the training set
m.Enet.cv.pred.train = predict(m.Enet.cv, X.train, s = m.Enet.cv$lambda.min)
print(paste('RMSE on training set:', rmse2(m.Enet.cv.pred.train, y.train)))
m.Enet.cv.pred.test = predict(m.Enet.cv, X.test, s = m.Enet.cv$lambda.min)
print(paste('RMSE on test set:', rmse2(m.Enet.cv.pred.test, y.test)))
model.outcome[['m.Enet.cv']] = list('model' = m.lasso.cv, 
                                    'rmse' =  data.frame('name'= 'Elastic net',
                                                         'train.RMSE' = rmse2(m.Enet.cv.pred.train, y.train),
                                                         'test.RMSE' = rmse2(m.Enet.cv.pred.test, y.test)))





m.Enet.cv_T <- cv.glmnet(X.test, y.test, alpha = 0.5)
Elastic_model_T <- glmnet(X.test,y.test ,  family = "gaussian",  alpha=0.5,  lambda = m.Enet.cv_T$lambda.min )
Elastic_model_T$dev.ratio

#R_squared for train data
m.Enet.cv_Tr <- cv.glmnet(X.train, y.train, alpha = 0.5)
Elastic_model_Tr <- glmnet(X.train,y.train ,  family = "gaussian",  alpha=0.5,  lambda = m.Enet.cv_Tr$lambda.min )
Elastic_model_Tr$dev.ratio



res.matrix = do.call(rbind, model.outcome)
# extract data frame object
res.df = do.call(rbind, res.matrix[,2]) 

library(ggplot2)
library(reshape2)


# melt to tidy data frame
df = melt(res.df, id.vars = "name", measure.vars = c("train.RMSE", "test.RMSE"))

# plot
ggplot(data=df, aes(x=name, y=value, fill=variable)) +
  geom_bar(stat="identity", position=position_dodge()) +
  scale_fill_hue(name="") +
  xlab("") + ylab("RMSE") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#_______________________________________________________________________________________________________________



